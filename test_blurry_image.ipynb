{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "604e8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from imageio import imread, imsave\n",
    "from skimage.transform import resize\n",
    "from skimage.util import img_as_float\n",
    "import numpy as np\n",
    "from path import Path\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "from models import DispNetS\n",
    "from utils import tensor2array\n",
    "from models import PoseExpNet\n",
    "import custom_transforms\n",
    "from utils import tensor2array, save_checkpoint, save_path_formatter, log_output_tensorboard\n",
    "import pdb\n",
    "from loss_functions import photometric_reconstruction_loss, explainability_loss, smooth_loss, blurry_loss\n",
    "from loss_functions import compute_depth_errors, compute_pose_errors\n",
    "from inverse_warp import *\n",
    "from logger import TermLogger, AverageMeter\n",
    "from tensorboardX import SummaryWriter\n",
    "from datasets.sequence_folders import SequenceFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a99769f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "disp_net = DispNetS().to(device)\n",
    "pose_exp_net = PoseExpNet(nb_ref_imgs=2, output_exp=False).to(device)\n",
    "weights = torch.load('checkpoints/kitti_raw_prepared,epoch_size3000,b64,m0.2/09-19-18:48/dispnet_model_best.pth.tar')\n",
    "disp_net.load_state_dict(weights['state_dict'])\n",
    "\n",
    "weights = torch.load('checkpoints/kitti_raw_prepared,epoch_size3000,b64,m0.2/09-19-18:48/exp_pose_model_best.pth.tar')\n",
    "pose_exp_net.load_state_dict(weights['state_dict'], strict=False)\n",
    "\n",
    "disp_net = torch.nn.DataParallel(disp_net,device_ids=[0,1,2])\n",
    "pose_exp_net = torch.nn.DataParallel(pose_exp_net,device_ids=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac67c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = custom_transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                            std=[0.5, 0.5, 0.5])\n",
    "train_transform = custom_transforms.Compose([\n",
    "    custom_transforms.RandomHorizontalFlip(),\n",
    "    custom_transforms.RandomScaleCrop(),\n",
    "    custom_transforms.ArrayToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "train_set = SequenceFolder(\n",
    "    'KITTI/kitti_raw_prepared/',\n",
    "    transform=train_transform,\n",
    "    seed=0,\n",
    "    train=True,\n",
    "    sequence_length=3\n",
    "    )\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=1, shuffle=True,\n",
    "        num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca0c8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-1,-1,-1],\n",
    "    std=[1/0.5, 1/0.5, 1/0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "545e1fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i, (tgt_img, ref_imgs, intrinsics, intrinsics_inv) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        tgt_img = tgt_img.to(device)\n",
    "        ref_imgs = [img.to(device) for img in ref_imgs]\n",
    "        intrinsics = intrinsics.to(device)\n",
    "\n",
    "        # compute output\n",
    "        disparities = disp_net(tgt_img)\n",
    "        depth = [1/disp for disp in disparities]\n",
    "  \n",
    "        depth_refs = []\n",
    "        for ref_img in ref_imgs:\n",
    "            disparities_refs = disp_net(ref_img)[0]\n",
    "            depth_refs.append(1/disparities_refs)\n",
    "        explainability_mask, pose = pose_exp_net(tgt_img, ref_imgs=ref_imgs)\n",
    "        blurry_img = blurry_image(ref_imgs,depth_refs, pose, intrinsics,'euler', 'zeros')\n",
    "\n",
    "        disparities_blurry = disp_net(blurry_img)\n",
    "        depth_blurry = [1/disp for disp in disparities_blurry]\n",
    "        explainability_mask_blurry, pose_blurry = pose_exp_net(target_image=blurry_img, blurry = True)\n",
    "        loss_4,warp,_ = blurry_loss(tgt_img, blurry_img, intrinsics, \n",
    "                            depth, explainability_mask_blurry, pose_blurry,\n",
    "                            'euler', 'zeros')\n",
    "        \n",
    "        blurry_img_ = inv_normalize(blurry_img)\n",
    "        target_img_ = inv_normalize(tgt_img)\n",
    "        warp,_ = inverse_warp(blurry_img,depth_blurry[0][:,0],pose_blurry[:,0],intrinsics)\n",
    "\n",
    "        save_image(blurry_img_, 'blurry/' + str(i) + '.png')\n",
    "        save_image(target_img_, 'blurry/' + str(i) + '_original.png')\n",
    "        save_image(inv_normalize(warp),'blurry/'+str(i) + '_warp.png')\n",
    "        if i > 30:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f4d303e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_blurry = result[-1][0]\n",
    "pose_blurry = result[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "224a9e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 416])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dbe105a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = inverse_warp(result[4],depth_blurry[:,0],pose_blurry[:,0],intrinsics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "87fd4911",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ = inv_normalize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0cf4c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(a_,'test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4c1ff466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "i = 0\n",
    "for result in results:\n",
    "    blurry_img.shape #torch.Size([64,3,28,28])\n",
    "    blurry_img_ = result[2][0] #torch.Size([3,28,28]\n",
    "    target_img_ = result[0][0]\n",
    "#     blurry_depth_ = \n",
    "#     target_depth_ = \n",
    "    \n",
    "    warp = result[-1][0]\n",
    "    # img1 = img1.numpy() # TypeError: tensor or list of tensors expected, got <class 'numpy.ndarray'>\n",
    "    save_image(blurry_img_, 'blurry/' + str(i) + '.png')\n",
    "    save_image(target_img_, 'blurry/' + str(i) + '_original.png')\n",
    "    save_image(warp,'blurry/'+ str(i) + '_warp.png')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4dfe037a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4844, -0.4670, -0.4471,  ...,  0.1483,  0.1522,  0.1328],\n",
       "         [-0.4740, -0.4789, -0.4819,  ...,  0.1733,  0.1839,  0.1601],\n",
       "         [-0.4793, -0.4649, -0.4704,  ...,  0.1532,  0.1717,  0.1827],\n",
       "         ...,\n",
       "         [-0.4134, -0.4127, -0.4106,  ..., -0.2205, -0.1676, -0.1525],\n",
       "         [-0.4071, -0.4041, -0.4017,  ..., -0.2750, -0.2662, -0.2363],\n",
       "         [-0.4007, -0.3985, -0.3934,  ..., -0.2883, -0.3140, -0.2975]],\n",
       "\n",
       "        [[-0.4530, -0.4369, -0.4231,  ...,  0.4229,  0.4296,  0.4538],\n",
       "         [-0.4444, -0.4526, -0.4570,  ...,  0.4195,  0.4300,  0.4675],\n",
       "         [-0.4479, -0.4336, -0.4395,  ...,  0.3739,  0.3968,  0.4583],\n",
       "         ...,\n",
       "         [-0.4029, -0.4000, -0.3976,  ..., -0.2303, -0.1841, -0.1724],\n",
       "         [-0.3956, -0.3923, -0.3881,  ..., -0.3058, -0.2971, -0.2573],\n",
       "         [-0.3892, -0.3868, -0.3782,  ..., -0.3241, -0.3381, -0.3111]],\n",
       "\n",
       "        [[-0.4452, -0.4304, -0.4227,  ...,  0.4710,  0.4709,  0.4713],\n",
       "         [-0.4384, -0.4499, -0.4556,  ...,  0.4689,  0.4733,  0.4929],\n",
       "         [-0.4401, -0.4258, -0.4322,  ...,  0.4104,  0.4281,  0.4979],\n",
       "         ...,\n",
       "         [-0.3827, -0.3792, -0.3769,  ..., -0.2306, -0.1887, -0.1808],\n",
       "         [-0.3729, -0.3693, -0.3671,  ..., -0.3064, -0.3002, -0.2653],\n",
       "         [-0.3628, -0.3593, -0.3546,  ..., -0.3283, -0.3486, -0.3284]]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blurry_img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1f889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='Inference script for DispNet learned with \\\n",
    "                                 Structure from Motion Learner inference on KITTI and CityScapes Dataset',\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\"--output-disp\", action='store_true', help=\"save disparity img\")\n",
    "parser.add_argument(\"--output-depth\", action='store_true', help=\"save depth img\")\n",
    "parser.add_argument(\"--pretrained\", required=True, type=str, help=\"pretrained DispNet path\")\n",
    "parser.add_argument(\"--img-height\", default=128, type=int, help=\"Image height\")\n",
    "parser.add_argument(\"--img-width\", default=416, type=int, help=\"Image width\")\n",
    "parser.add_argument(\"--no-resize\", action='store_true', help=\"no resizing is done\")\n",
    "\n",
    "parser.add_argument(\"--dataset-list\", default=None, type=str, help=\"Dataset list file\")\n",
    "parser.add_argument(\"--dataset-dir\", default='.', type=str, help=\"Dataset directory\")\n",
    "parser.add_argument(\"--output-dir\", default='output', type=str, help=\"Output directory\")\n",
    "\n",
    "parser.add_argument(\"--img-exts\", default=['png', 'jpg', 'bmp'], nargs='*', type=str, help=\"images extensions to glob\")\n",
    "try:\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "except:\n",
    "    pdb.set_trace()\n",
    "@torch.no_grad()\n",
    "def main():\n",
    "    args = parser.parse_args()\n",
    "    if not(args.output_disp or args.output_depth):\n",
    "        print('You must at least output one value !')\n",
    "        return\n",
    "\n",
    "    disp_net = DispNetS().to(device)\n",
    "    weights = torch.load(args.pretrained)\n",
    "    disp_net.load_state_dict(weights['state_dict'])\n",
    "    disp_net.eval()\n",
    "\n",
    "    dataset_dir = Path(args.dataset_dir)\n",
    "    output_dir = Path(args.output_dir)\n",
    "    output_dir.makedirs_p()\n",
    "\n",
    "    if args.dataset_list is not None:\n",
    "        with open(args.dataset_list, 'r') as f:\n",
    "            test_files = [dataset_dir/file for file in f.read().splitlines()]\n",
    "    else:\n",
    "        test_files = sum([list(dataset_dir.walkfiles('*.{}'.format(ext))) for ext in args.img_exts], [])\n",
    "\n",
    "    print('{} files to test'.format(len(test_files)))\n",
    "\n",
    "    for file in tqdm(test_files):\n",
    "\n",
    "        img = img_as_float(imread(file))\n",
    "\n",
    "        h,w,_ = img.shape\n",
    "        if (not args.no_resize) and (h != args.img_height or w != args.img_width):\n",
    "            img = resize(img, (args.img_height, args.img_width))\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "\n",
    "        tensor_img = torch.from_numpy(img.astype(np.float32)).unsqueeze(0)\n",
    "        tensor_img = ((tensor_img - 0.5)/0.5).to(device)\n",
    "\n",
    "        output = disp_net(tensor_img)[0]\n",
    "\n",
    "        file_path, file_ext = file.relpath(args.dataset_dir).splitext()\n",
    "        file_name = '-'.join(file_path.splitall()[1:])\n",
    "\n",
    "        if args.output_disp:\n",
    "            disp = (255*tensor2array(output, max_value=None, colormap='bone')).astype(np.uint8)\n",
    "            imsave(output_dir/'{}_disp{}'.format(file_name, file_ext), np.transpose(disp, (1,2,0)))\n",
    "        if args.output_depth:\n",
    "            depth = 1/output\n",
    "            depth = (255*tensor2array(depth, max_value=None, colormap='rainbow')).astype(np.uint8)\n",
    "            imsave(output_dir/'{}_depth{}'.format(file_name, file_ext), np.transpose(depth, (1,2,0)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f32b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7addfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be433a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380eb086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924bb385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886a593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a85bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea5a78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ac5dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
