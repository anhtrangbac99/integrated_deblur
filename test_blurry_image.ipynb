{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6fe594b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from imageio import imread, imsave\n",
    "from skimage.transform import resize\n",
    "from skimage.util import img_as_float\n",
    "import numpy as np\n",
    "from path import Path\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "from models import DispNetS\n",
    "from utils import tensor2array\n",
    "from models import PoseExpNet\n",
    "import custom_transforms\n",
    "from utils import tensor2array, save_checkpoint, save_path_formatter, log_output_tensorboard\n",
    "import pdb\n",
    "from loss_functions import photometric_reconstruction_loss, explainability_loss, smooth_loss, blurry_loss\n",
    "from loss_functions import compute_depth_errors, compute_pose_errors\n",
    "from inverse_warp import *\n",
    "from logger import TermLogger, AverageMeter\n",
    "from tensorboardX import SummaryWriter\n",
    "from datasets.sequence_folders import SequenceFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a64ebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "disp_net = DispNetS().to(device)\n",
    "pose_exp_net = PoseExpNet(nb_ref_imgs=2, output_exp=False).to(device)\n",
    "weights = torch.load('checkpoints/kitti_raw_prepared,epoch_size3000,b64,m0.2/09-19-18:48/dispnet_model_best.pth.tar')\n",
    "disp_net.load_state_dict(weights['state_dict'])\n",
    "\n",
    "weights = torch.load('checkpoints/kitti_raw_prepared,epoch_size3000,b64,m0.2/09-19-18:48/exp_pose_model_best.pth.tar')\n",
    "pose_exp_net.load_state_dict(weights['state_dict'], strict=False)\n",
    "\n",
    "disp_net = torch.nn.DataParallel(disp_net,device_ids=[0,1,2])\n",
    "pose_exp_net = torch.nn.DataParallel(pose_exp_net,device_ids=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c25687ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = custom_transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                            std=[0.5, 0.5, 0.5])\n",
    "train_transform = custom_transforms.Compose([\n",
    "    custom_transforms.RandomHorizontalFlip(),\n",
    "    custom_transforms.RandomScaleCrop(),\n",
    "    custom_transforms.ArrayToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "train_set = SequenceFolder(\n",
    "    'KITTI/kitti_raw_prepared/',\n",
    "    transform=train_transform,\n",
    "    seed=0,\n",
    "    train=True,\n",
    "    sequence_length=3\n",
    "    )\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_set, batch_size=1, shuffle=True,\n",
    "        num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3694cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-1,-1,-1],\n",
    "    std=[1/0.5, 1/0.5, 1/0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "26ad197b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, (tgt_img, ref_imgs, intrinsics, intrinsics_inv) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        tgt_img = tgt_img.to(device)\n",
    "        ref_imgs = [img.to(device) for img in ref_imgs]\n",
    "        intrinsics = intrinsics.to(device)\n",
    "\n",
    "        # compute output\n",
    "        disparities = disp_net(tgt_img)\n",
    "        depth = [1/disp for disp in disparities]\n",
    "  \n",
    "        depth_refs = []\n",
    "        for ref_img in ref_imgs:\n",
    "            disparities_refs = disp_net(ref_img)[0]\n",
    "            depth_refs.append(1/disparities_refs)\n",
    "        explainability_mask, pose = pose_exp_net(tgt_img, ref_imgs=ref_imgs)\n",
    "        blurry_img,valid_points = blurry_image(ref_imgs,depth_refs, pose, intrinsics,'euler', 'border')\n",
    "#         blurry_img = blurry_img * valid_points.unsqueeze(1).float()\n",
    "        disparities_blurry = disp_net(blurry_img)\n",
    "        depth_blurry = [1/disp for disp in disparities_blurry]\n",
    "        explainability_mask_blurry, pose_blurry = pose_exp_net(target_image=blurry_img, blurry = True)\n",
    "      \n",
    "        blurry_img_ = inv_normalize(blurry_img)\n",
    "        target_img_ = inv_normalize(tgt_img)\n",
    "        warp,_ = inverse_warp(blurry_img,depth_blurry[0][:,0],pose_blurry[:,0],intrinsics)\n",
    "\n",
    "        save_image(blurry_img_, 'blurry/' + str(i) + '.png')\n",
    "        save_image(target_img_, 'blurry/' + str(i) + '_original.png')\n",
    "        save_image(inv_normalize(warp),'blurry/'+str(i) + '_warp.png')\n",
    "        disp = (255*tensor2array(disparities[0], max_value=None, colormap='bone')).astype(np.uint8)\n",
    "        imageio.imsave('blurry/'+str(i) + '_disp.png', np.transpose(disp, (1,2,0)))\n",
    "        depth = (255*tensor2array(depth[0], max_value=None, colormap='rainbow')).astype(np.uint8)\n",
    "        imageio.imsave('blurry/'+str(i) + '_depth.png', np.transpose(depth, (1,2,0)))\n",
    "        for j in range(len(ref_imgs)):\n",
    "            save_image(inv_normalize(ref_imgs[j]),'blurry/'+str(i) + '_ref_'+str(j)+'.png')\n",
    "       \n",
    "        if i > 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4c8292e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cam_coords = pixel2cam(depth[0][:,0], intrinsics.inverse())  # [B,3,H,W]\n",
    "pose_mat = pose_vec2mat(pose_blurry[:,0], 'euler')  # [B,3,4]\n",
    "# Get projection matrix for tgt camera frame to source pixel frame\n",
    "proj_cam_to_src_pixel = intrinsics @ pose_mat  # [B, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9811a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rot, tr = proj_cam_to_src_pixel[..., :3], proj_cam_to_src_pixel[..., -1:]\n",
    "src_pixel_coords = cam2pixel(cam_coords, rot.inverse(), )  # [B,H,W,2]\n",
    "\n",
    "projected_img = F.grid_sample(blurry_img, src_pixel_coords, padding_mode='border', align_corners=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9009e460",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(inv_normalize(projected_img),'test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "57eb13e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.0007, -1.0007],\n",
       "          [-1.0010, -1.0010],\n",
       "          [-1.0012, -1.0013],\n",
       "          ...,\n",
       "          [-0.9986, -1.0011],\n",
       "          [-0.9988, -1.0009],\n",
       "          [-0.9991, -1.0007]],\n",
       "\n",
       "         [[-1.0008, -1.0008],\n",
       "          [-1.0011, -1.0011],\n",
       "          [-1.0013, -1.0013],\n",
       "          ...,\n",
       "          [-0.9986, -1.0011],\n",
       "          [-0.9988, -1.0009],\n",
       "          [-0.9991, -1.0007]],\n",
       "\n",
       "         [[-1.0008, -1.0008],\n",
       "          [-1.0011, -1.0011],\n",
       "          [-1.0013, -1.0013],\n",
       "          ...,\n",
       "          [-0.9986, -1.0011],\n",
       "          [-0.9988, -1.0009],\n",
       "          [-0.9990, -1.0007]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.0004, -0.9995],\n",
       "          [-1.0005, -0.9995],\n",
       "          [-1.0005, -0.9995],\n",
       "          ...,\n",
       "          [-0.9994, -0.9994],\n",
       "          [-0.9993, -0.9994],\n",
       "          [-0.9993, -0.9994]],\n",
       "\n",
       "         [[-1.0004, -0.9995],\n",
       "          [-1.0005, -0.9995],\n",
       "          [-1.0005, -0.9995],\n",
       "          ...,\n",
       "          [-0.9993, -0.9994],\n",
       "          [-0.9992, -0.9993],\n",
       "          [-0.9993, -0.9994]],\n",
       "\n",
       "         [[-1.0004, -0.9995],\n",
       "          [-1.0005, -0.9994],\n",
       "          [-1.0005, -0.9995],\n",
       "          ...,\n",
       "          [-0.9993, -0.9994],\n",
       "          [-0.9992, -0.9993],\n",
       "          [-0.9992, -0.9993]]]], device='cuda:0',\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_pixel_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20307733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.9663, -1.0306],\n",
       "          [-0.9670, -1.0588],\n",
       "          [-0.9667, -1.0817],\n",
       "          ...,\n",
       "          [ 1.0139, -1.0786],\n",
       "          [ 1.0163, -1.0649],\n",
       "          [ 1.0168, -1.0405]],\n",
       "\n",
       "         [[-0.9699, -1.0341],\n",
       "          [-0.9696, -1.0569],\n",
       "          [-0.9669, -1.0680],\n",
       "          ...,\n",
       "          [ 1.0142, -1.0656],\n",
       "          [ 1.0166, -1.0514],\n",
       "          [ 1.0160, -1.0211]],\n",
       "\n",
       "         [[-0.9681, -1.0097],\n",
       "          [-0.9691, -1.0394],\n",
       "          [-0.9670, -1.0529],\n",
       "          ...,\n",
       "          [ 1.0141, -1.0501],\n",
       "          [ 1.0165, -1.0361],\n",
       "          [ 1.0173, -1.0134]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.9410,  0.9328],\n",
       "          [-0.9375,  0.9291],\n",
       "          [-0.9345,  0.9238],\n",
       "          ...,\n",
       "          [ 0.9763,  0.9467],\n",
       "          [ 0.9832,  0.9390],\n",
       "          [ 0.9885,  0.9371]],\n",
       "\n",
       "         [[-0.9401,  0.9502],\n",
       "          [-0.9380,  0.9420],\n",
       "          [-0.9343,  0.9394],\n",
       "          ...,\n",
       "          [ 0.9770,  0.9591],\n",
       "          [ 0.9862,  0.9428],\n",
       "          [ 0.9889,  0.9501]],\n",
       "\n",
       "         [[-0.9395,  0.9670],\n",
       "          [-0.9403,  0.9489],\n",
       "          [-0.9333,  0.9573],\n",
       "          ...,\n",
       "          [ 0.9768,  0.9743],\n",
       "          [ 0.9857,  0.9591],\n",
       "          [ 0.9922,  0.9531]]]], device='cuda:0',\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_pixel_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94e2ebdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 416])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1529891a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "linalg.inv: A must be batches of square matrices, but they are 3 by 4 matrices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: linalg.inv: A must be batches of square matrices, but they are 3 by 4 matrices"
     ]
    }
   ],
   "source": [
    "a.inverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c1ba356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0054, -0.0087, -0.0062, -0.0454, -0.0115, -0.0043]],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1*pose_blurry[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a457694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0054, 0.0087, 0.0062, 0.0454, 0.0115, 0.0043]], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose_blurry[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92018c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9056fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "536e21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_blurry = result[-1][0]\n",
    "pose_blurry = result[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee1e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "de75f811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 416])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "05442c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = inverse_warp(result[4],depth_blurry[:,0],pose_blurry[:,0],intrinsics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "194ffd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ = inv_normalize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0dd566db",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(a_,'test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c9c200d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "i = 0\n",
    "for result in results:\n",
    "    blurry_img.shape #torch.Size([64,3,28,28])\n",
    "    blurry_img_ = result[2][0] #torch.Size([3,28,28]\n",
    "    target_img_ = result[0][0]\n",
    "#     blurry_depth_ = \n",
    "#     target_depth_ = \n",
    "    \n",
    "    warp = result[-1][0]\n",
    "    # img1 = img1.numpy() # TypeError: tensor or list of tensors expected, got <class 'numpy.ndarray'>\n",
    "    save_image(blurry_img_, 'blurry/' + str(i) + '.png')\n",
    "    save_image(target_img_, 'blurry/' + str(i) + '_original.png')\n",
    "    save_image(warp,'blurry/'+ str(i) + '_warp.png')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f40af7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4844, -0.4670, -0.4471,  ...,  0.1483,  0.1522,  0.1328],\n",
       "         [-0.4740, -0.4789, -0.4819,  ...,  0.1733,  0.1839,  0.1601],\n",
       "         [-0.4793, -0.4649, -0.4704,  ...,  0.1532,  0.1717,  0.1827],\n",
       "         ...,\n",
       "         [-0.4134, -0.4127, -0.4106,  ..., -0.2205, -0.1676, -0.1525],\n",
       "         [-0.4071, -0.4041, -0.4017,  ..., -0.2750, -0.2662, -0.2363],\n",
       "         [-0.4007, -0.3985, -0.3934,  ..., -0.2883, -0.3140, -0.2975]],\n",
       "\n",
       "        [[-0.4530, -0.4369, -0.4231,  ...,  0.4229,  0.4296,  0.4538],\n",
       "         [-0.4444, -0.4526, -0.4570,  ...,  0.4195,  0.4300,  0.4675],\n",
       "         [-0.4479, -0.4336, -0.4395,  ...,  0.3739,  0.3968,  0.4583],\n",
       "         ...,\n",
       "         [-0.4029, -0.4000, -0.3976,  ..., -0.2303, -0.1841, -0.1724],\n",
       "         [-0.3956, -0.3923, -0.3881,  ..., -0.3058, -0.2971, -0.2573],\n",
       "         [-0.3892, -0.3868, -0.3782,  ..., -0.3241, -0.3381, -0.3111]],\n",
       "\n",
       "        [[-0.4452, -0.4304, -0.4227,  ...,  0.4710,  0.4709,  0.4713],\n",
       "         [-0.4384, -0.4499, -0.4556,  ...,  0.4689,  0.4733,  0.4929],\n",
       "         [-0.4401, -0.4258, -0.4322,  ...,  0.4104,  0.4281,  0.4979],\n",
       "         ...,\n",
       "         [-0.3827, -0.3792, -0.3769,  ..., -0.2306, -0.1887, -0.1808],\n",
       "         [-0.3729, -0.3693, -0.3671,  ..., -0.3064, -0.3002, -0.2653],\n",
       "         [-0.3628, -0.3593, -0.3546,  ..., -0.3283, -0.3486, -0.3284]]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blurry_img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b045de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='Inference script for DispNet learned with \\\n",
    "                                 Structure from Motion Learner inference on KITTI and CityScapes Dataset',\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\"--output-disp\", action='store_true', help=\"save disparity img\")\n",
    "parser.add_argument(\"--output-depth\", action='store_true', help=\"save depth img\")\n",
    "parser.add_argument(\"--pretrained\", required=True, type=str, help=\"pretrained DispNet path\")\n",
    "parser.add_argument(\"--img-height\", default=128, type=int, help=\"Image height\")\n",
    "parser.add_argument(\"--img-width\", default=416, type=int, help=\"Image width\")\n",
    "parser.add_argument(\"--no-resize\", action='store_true', help=\"no resizing is done\")\n",
    "\n",
    "parser.add_argument(\"--dataset-list\", default=None, type=str, help=\"Dataset list file\")\n",
    "parser.add_argument(\"--dataset-dir\", default='.', type=str, help=\"Dataset directory\")\n",
    "parser.add_argument(\"--output-dir\", default='output', type=str, help=\"Output directory\")\n",
    "\n",
    "parser.add_argument(\"--img-exts\", default=['png', 'jpg', 'bmp'], nargs='*', type=str, help=\"images extensions to glob\")\n",
    "try:\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "except:\n",
    "    pdb.set_trace()\n",
    "@torch.no_grad()\n",
    "def main():\n",
    "    args = parser.parse_args()\n",
    "    if not(args.output_disp or args.output_depth):\n",
    "        print('You must at least output one value !')\n",
    "        return\n",
    "\n",
    "    disp_net = DispNetS().to(device)\n",
    "    weights = torch.load(args.pretrained)\n",
    "    disp_net.load_state_dict(weights['state_dict'])\n",
    "    disp_net.eval()\n",
    "\n",
    "    dataset_dir = Path(args.dataset_dir)\n",
    "    output_dir = Path(args.output_dir)\n",
    "    output_dir.makedirs_p()\n",
    "\n",
    "    if args.dataset_list is not None:\n",
    "        with open(args.dataset_list, 'r') as f:\n",
    "            test_files = [dataset_dir/file for file in f.read().splitlines()]\n",
    "    else:\n",
    "        test_files = sum([list(dataset_dir.walkfiles('*.{}'.format(ext))) for ext in args.img_exts], [])\n",
    "\n",
    "    print('{} files to test'.format(len(test_files)))\n",
    "\n",
    "    for file in tqdm(test_files):\n",
    "\n",
    "        img = img_as_float(imread(file))\n",
    "\n",
    "        h,w,_ = img.shape\n",
    "        if (not args.no_resize) and (h != args.img_height or w != args.img_width):\n",
    "            img = resize(img, (args.img_height, args.img_width))\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "\n",
    "        tensor_img = torch.from_numpy(img.astype(np.float32)).unsqueeze(0)\n",
    "        tensor_img = ((tensor_img - 0.5)/0.5).to(device)\n",
    "\n",
    "        output = disp_net(tensor_img)[0]\n",
    "\n",
    "        file_path, file_ext = file.relpath(args.dataset_dir).splitext()\n",
    "        file_name = '-'.join(file_path.splitall()[1:])\n",
    "\n",
    "        if args.output_disp:\n",
    "            disp = (255*tensor2array(output, max_value=None, colormap='bone')).astype(np.uint8)\n",
    "            imsave(output_dir/'{}_disp{}'.format(file_name, file_ext), np.transpose(disp, (1,2,0)))\n",
    "        if args.output_depth:\n",
    "            depth = 1/output\n",
    "            depth = (255*tensor2array(depth, max_value=None, colormap='rainbow')).astype(np.uint8)\n",
    "            imsave(output_dir/'{}_depth{}'.format(file_name, file_ext), np.transpose(depth, (1,2,0)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8fad28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18968a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3591a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05736ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7613952e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec82a4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac1bc45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ef2455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530a38a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
